{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d9b095a1-e638-4871-ad25-ad0416c5b482",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7860\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import gradio as gr\n",
    "import numpy as np\n",
    "import cv2\n",
    "import mediapipe as mp\n",
    "import base64\n",
    "import tensorflow as tf\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "import json\n",
    "\n",
    "mp_holistic = mp.solutions.holistic\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "def load_model(model_path):\n",
    "    return tf.keras.models.load_model(model_path)\n",
    "\n",
    "models = {\n",
    "    'Getting Started': load_model('getting_started_v2.h5'),\n",
    "    'Polite Expressions': load_model('polite_expressions.h5'),\n",
    "    'Questions': load_model('questions.h5'),\n",
    "    'Responses': load_model('responses.h5'),\n",
    "    'Compliments': load_model('compliments.h5'),\n",
    "    'Feelings': load_model('feelings.h5'),\n",
    "    'Objects': load_model('objects.h5'),\n",
    "    'Animals': load_model('animals.h5'),\n",
    "    'Fruits': load_model('fruits.h5'),\n",
    "    'The Sky': load_model('the_sky.h5'),\n",
    "}\n",
    "\n",
    "model_classes = {\n",
    "    'Getting Started': ['Hello', 'Goodbye', 'Dad', 'Mom', 'I love you'],\n",
    "    'Polite Expressions': ['Please', 'Excuse Me', 'Thank You', 'Sorry', 'You are Welcome'],\n",
    "    'Questions': ['Question', 'Where', 'Who', 'Why', 'What'],\n",
    "    'Responses': ['Yes', 'No', 'Now', 'Later', 'Tomorrow'],\n",
    "    'Compliments': ['Beautiful', 'Cute', 'Nice', 'Funny', 'Smart'],      \n",
    "    'Feelings': ['Happy', 'Sad', 'Proud', 'Excited', 'Hungry'],\n",
    "    'Objects': ['Computer', 'Phone', 'Camera', 'Bag', 'Toothbrush'],\n",
    "    'Animals': ['Llama', 'Horse', 'Cat', 'Pig', 'Goat'],\n",
    "    'The Sky': ['Sky', 'Sun', 'Moon', 'Clouds', 'Stars'],\n",
    "    'Fruits': ['Fruit', 'Apple', 'Orange', 'Strawberry', 'Grapes'],\n",
    "}\n",
    "\n",
    "def extract_keypoints(results, frame_width, frame_height):\n",
    "    pose = np.array([[landmark.x, landmark.y, landmark.z, landmark.visibility] for landmark in results.pose_landmarks.landmark]).flatten() if results.pose_landmarks else np.zeros(33*4)\n",
    "    lh = np.array([[landmark.x, landmark.y, landmark.z] for landmark in results.left_hand_landmarks.landmark]).flatten() if results.left_hand_landmarks else np.zeros(21*3)\n",
    "    rh = np.array([[landmark.x, landmark.y, landmark.z] for landmark in results.right_hand_landmarks.landmark]).flatten() if results.right_hand_landmarks else np.zeros(21*3)\n",
    "\n",
    "    # Normalization\n",
    "    pose[::4] = pose[::4] / frame_width\n",
    "    pose[1::4] = pose[1::4] / frame_height\n",
    "    lh[::3] = lh[::3] / frame_width\n",
    "    lh[1::3] = lh[1::3] / frame_height\n",
    "    rh[::3] = rh[::3] / frame_width\n",
    "    rh[1::4] = rh[1::4] / frame_height\n",
    "\n",
    "    return np.concatenate([pose, lh, rh])\n",
    "\n",
    "def process_image_data_uri(data_uri):\n",
    "    # Convert base64 to PIL Image\n",
    "    header, encoded = data_uri.split(\",\", 1)\n",
    "    image_data = base64.b64decode(encoded)\n",
    "    pil_image = Image.open(BytesIO(image_data))\n",
    "    return cv2.cvtColor(np.array(pil_image), cv2.COLOR_RGB2BGR)\n",
    "\n",
    "def is_all_masked(sequence, mask_value=0):\n",
    "    \"\"\"Check if the entire sequence is masked.\"\"\"\n",
    "    return (sequence == mask_value).all()\n",
    "\n",
    "def extract_keypoints_from_sequence(image_data_uris):\n",
    "    keypoints_sequence = []\n",
    "    holistic = mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5)\n",
    "    \n",
    "    for data_uri in image_data_uris:\n",
    "        image = process_image_data_uri(data_uri)\n",
    "\n",
    "        # MediaPipe keypoint extraction\n",
    "        results = holistic.process(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
    "        keypoints = extract_keypoints(results, image.shape[1], image.shape[0])\n",
    "        keypoints_sequence.append(keypoints)\n",
    "        \n",
    "    holistic.close()\n",
    "    return np.array(keypoints_sequence)\n",
    "\n",
    "def predict_sequence(model_name, image_data_uris_json):\n",
    "    image_data_uris = json.loads(image_data_uris_json)\n",
    "    keypoints_sequence = extract_keypoints_from_sequence(image_data_uris)\n",
    "\n",
    "    if is_all_masked(keypoints_sequence):\n",
    "        return \"Nothing to detect. Try Again\"\n",
    "    else:\n",
    "        try:\n",
    "            # Shaped as [1, 40, 258]\n",
    "            if keypoints_sequence.shape[0] == 40 and keypoints_sequence.shape[1] == 258:\n",
    "                keypoints_sequence = np.expand_dims(keypoints_sequence, axis=0)\n",
    "            else:\n",
    "                # Handle incorrect shape\n",
    "                return \"Incorrect keypoints sequence shape. Expected 40 sets of 258 keypoints.\"\n",
    "    \n",
    "            current_model = models[model_name]\n",
    "            if current_model is None:\n",
    "                return \"Model not found.\"\n",
    "            \n",
    "            actions = model_classes[model_name]\n",
    "            res = current_model.predict(keypoints_sequence)[0]\n",
    "            top_action_indices = np.argsort(res)[-3:][::-1]\n",
    "            top_actions = [(actions[index], res[index]*100) for index in top_action_indices if res[index]*100 > 30]\n",
    "            \n",
    "            if not top_actions: \n",
    "                return \"Try again\"\n",
    "            else:\n",
    "                return ', '.join([f\"{name} ({prob:.2f}%)\" for name, prob in top_actions])\n",
    "        except Exception as e:\n",
    "            return f\"Error processing sequence: {str(e)}\"\n",
    "        \n",
    "# Gradio interface\n",
    "iface = gr.Interface(\n",
    "    fn=predict_sequence,\n",
    "    inputs=[\n",
    "        gr.Dropdown(label=\"Model Selection\", choices=list(models.keys())),\n",
    "        gr.Textbox(label=\"Enter Image Data URIs Here (JSON array)\")\n",
    "    ],\n",
    "    outputs=gr.Text(label=\"Predicted Action\"),\n",
    "    title=\"Hand Sign Prediction with aslmodel_v2\"\n",
    ")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    iface.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2d90a242-be2d-461e-bce2-717e7ca9decd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Closing server running on port: 7860\n"
     ]
    }
   ],
   "source": [
    "iface.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
